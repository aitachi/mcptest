import asyncio
from typing import Dict, Any, List
from .llm_client import LLMClient
from .executor import TaskExecutor
from config import logger


class Agent:
    def __init__(self, mcp_server):
        self.mcp_server = mcp_server
        self.llm_client = LLMClient()
        self.executor = TaskExecutor(mcp_server, self.llm_client)
        logger.info("ğŸ¤– Agentåˆå§‹åŒ–å®Œæˆ")

    async def process_query(self, query: str) -> Dict[str, Any]:
        logger.info(f"ğŸ¯ Agentæ¥æ”¶åˆ°ç”¨æˆ·æŸ¥è¯¢: {query}")
        print(f"ğŸ¤– Agent æ”¶åˆ°æŸ¥è¯¢: {query}")

        logger.info("ğŸ“‹ è·å–MCPæœåŠ¡å™¨å¯ç”¨åŠŸèƒ½åˆ—è¡¨...")
        print("\nğŸ” è·å–å¯ç”¨åŠŸèƒ½...")
        available_functions = self.mcp_server.get_available_functions()
        logger.info(f"ğŸ“¦ è·å–åˆ° {len(available_functions)} ä¸ªå¯ç”¨åŠŸèƒ½")

        for func in available_functions:
            logger.info(f"  - {func['name']}: {func['description']}")

        logger.info("ğŸ§  è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œä»»åŠ¡åˆ†æ...")
        print("\nğŸ§  å¤§æ¨¡å‹åˆ†æä»»åŠ¡...")
        task_analysis = await self.llm_client.analyze_task(query, available_functions)

        if not task_analysis.get("steps"):
            error_msg = "å¤§æ¨¡å‹æ— æ³•ç”Ÿæˆæ‰§è¡Œè®¡åˆ’"
            logger.error(f"âŒ {error_msg}")
            return {
                "success": False,
                "message": error_msg,
                "query": query
            }

        steps = task_analysis["steps"]
        reasoning = task_analysis.get("reasoning", "")

        logger.info(f"ğŸ“‹ å¤§æ¨¡å‹ç”Ÿæˆæ‰§è¡Œè®¡åˆ’æˆåŠŸ")
        logger.info(f"ğŸ“Š è®¡åˆ’æ­¥éª¤æ•°: {len(steps)}")
        logger.info(f"ğŸ’­ åˆ†æç†ç”±: {reasoning}")

        print(f"ğŸ“‹ å¤§æ¨¡å‹ç”Ÿæˆæ‰§è¡Œè®¡åˆ’ï¼Œå…± {len(steps)} ä¸ªæ­¥éª¤:")
        for i, step in enumerate(steps, 1):
            print(f"  {i}. {step['function_name']} - {step['parameters']}")
            logger.info(f"  æ­¥éª¤{i}: {step['function_name']} - å‚æ•°: {step['parameters']}")
        print(f"ğŸ’­ åˆ†æç†ç”±: {reasoning}")

        logger.info("âš¡ å¼€å§‹æ‰§è¡Œä»»åŠ¡è®¡åˆ’...")
        results = await self.executor.execute_with_llm_guidance(query, steps)

        summary = self._generate_summary(query, steps, results)
        logger.info(f"ğŸ“Š ç”Ÿæˆæ‰§è¡Œæ€»ç»“: {summary}")

        return {
            "success": True,
            "query": query,
            "steps": steps,
            "reasoning": reasoning,
            "results": results,
            "summary": summary
        }

    def _generate_summary(self, query: str, steps: List[Dict], results: List[Dict]) -> Dict[str, Any]:
        successful_steps = [r for r in results if r["result"].get("success")]
        failed_steps = [r for r in results if not r["result"].get("success")]

        total_time = sum(r["execution_time"] for r in results)

        summary = {
            "total_planned_steps": len(steps),
            "total_executed_steps": len(results),
            "successful_steps": len(successful_steps),
            "failed_steps": len(failed_steps),
            "total_execution_time": total_time,
            "average_step_time": total_time / len(results) if results else 0,
            "success_rate": len(successful_steps) / len(results) if results else 0,
            "status": "completed" if not failed_steps else "partial_success"
        }

        logger.info(
            f"ğŸ“ˆ æ‰§è¡Œç»Ÿè®¡: è®¡åˆ’{summary['total_planned_steps']}æ­¥, æ‰§è¡Œ{summary['total_executed_steps']}æ­¥, æˆåŠŸ{summary['successful_steps']}æ­¥, å¤±è´¥{summary['failed_steps']}æ­¥")
        logger.info(
            f"â±ï¸  æ€»è€—æ—¶: {summary['total_execution_time']:.2f}ç§’, å¹³å‡æ¯æ­¥: {summary['average_step_time']:.2f}ç§’")
        logger.info(f"ğŸ“Š æˆåŠŸç‡: {summary['success_rate']:.1%}")

        return summary